{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Data Analysis with Deep Learning\n",
    "\n",
    "This notebook demonstrates the use of deep learning for medical data analysis, supporting both **medical images** and **ECG signals** with classification and regression tasks. \n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data preprocessing** (image enhancement or ECG signal processing)\n",
    "2. **Dataset creation and data loading** (with proper train/val/test splits)\n",
    "3. **Model architecture definition** (2D CNN for images, 1D CNN for ECG)\n",
    "4. **Hyperparameter tuning** (grid search optimization)\n",
    "5. **Model training and evaluation** (with early stopping and metrics)\n",
    "6. **Results visualization** (training curves, confusion matrices, etc.)\n",
    "\n",
    "## Key Features:\n",
    "- **Unified interface** for both image and ECG data\n",
    "- **Modular design** with separate preprocessing, dataset, and model modules\n",
    "- **Robust ECG processing** including PhysioNet format support\n",
    "- **Comprehensive evaluation** with classification and regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ AutoDL æœ€ç»ˆç¯å¢ƒä¸æ•°æ®å‡†å¤‡è„šæœ¬å¼€å§‹...\n",
      "\n",
      "æ­¥éª¤1ï¼šæ­£åœ¨ä» requirement2.txt å®‰è£…æ‰€æœ‰ä¾èµ–åº“...\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ… ä¾èµ–åº“å®‰è£…å®Œæˆï¼\n",
      "\n",
      "æ­¥éª¤2ï¼šæ­£åœ¨å‡†å¤‡ç›®æ ‡æ–‡ä»¶å¤¹: ./data/ori_images\n",
      "æ–‡ä»¶å¤¹éç©ºï¼Œæ­£åœ¨æ¸…ç©ºæ—§å›¾ç‰‡...\n",
      "æ—§å›¾ç‰‡å·²æ¸…ç©ºã€‚\n",
      "\n",
      "æ­¥éª¤3ï¼šæ­£åœ¨ä» ../data/boneage-train.csv è¯»å–å¹¶å¤„ç†æ ‡ç­¾...\n",
      "âœ… æˆåŠŸåˆ›å»ºå¹¶ä¿å­˜äº†åŒ…å« 5000 æ¡è®°å½•çš„ ./data/labels.csv\n",
      "\n",
      "æ­¥éª¤4ï¼šå‡†å¤‡æ ¹æ®æ ‡ç­¾æ–‡ä»¶å¤åˆ¶ 5000 å¼ å›¾ç‰‡...\n",
      "\n",
      "--- âœ… æ•°æ®å‡†å¤‡å®Œæˆ ---\n",
      "æˆåŠŸå¤åˆ¶äº† 5000 å¼ å›¾ç‰‡ã€‚\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# âœ…âœ…âœ… æœ€ç»ˆçš„ã€ä¸€åŠ³æ°¸é€¸çš„å¯åŠ¨è„šæœ¬ (æ›¿æ¢ä½ çš„ç¬¬ä¸€ä¸ªcell)\n",
    "# =======================================================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸš€ AutoDL æœ€ç»ˆç¯å¢ƒä¸æ•°æ®å‡†å¤‡è„šæœ¬å¼€å§‹...\")\n",
    "\n",
    "# --- ä¿®æ­£1ï¼šæŠŠå®‰è£…ä¾èµ–æ”¾åœ¨æ‰€æœ‰æ“ä½œçš„æœ€å‰é¢ï¼---\n",
    "# è¿™æ ·å¯ä»¥ä»æ ¹æºä¸Šè§£å†³ \"ModuleNotFoundError: No module named 'pandas'\" çš„é—®é¢˜\n",
    "requirements_file = 'requirement2.txt'\n",
    "if os.path.exists(requirements_file):\n",
    "    print(f\"\\næ­¥éª¤1ï¼šæ­£åœ¨ä» {requirements_file} å®‰è£…æ‰€æœ‰ä¾èµ–åº“...\")\n",
    "    !pip install -q -r {requirements_file}\n",
    "    print(\"âœ… ä¾èµ–åº“å®‰è£…å®Œæˆï¼\")\n",
    "else:\n",
    "    print(f\"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä¾èµ–æ–‡ä»¶ {requirements_file}ï¼Œè·³è¿‡å®‰è£…ã€‚\")\n",
    "\n",
    "# --- ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°å¯¼å…¥åˆšåˆšå®‰è£…å¥½çš„åº“äº† ---\n",
    "import pandas as pd\n",
    "\n",
    "# --- ä¿®æ­£2ï¼šå®šä¹‰äº‘ç«¯æœåŠ¡å™¨ä¸Šçš„æ­£ç¡®ç›¸å¯¹è·¯å¾„ ---\n",
    "# æˆ‘ä»¬æ‰€æœ‰çš„ä»£ç éƒ½åœ¨ 'æ‰€æœ‰ä»£ç _forGPU' æ–‡ä»¶å¤¹é‡Œè¿è¡Œ\n",
    "# è€Œæ•°æ®åœ¨æˆ‘ä»¬æ‰‹åŠ¨æ•´ç†åï¼Œæ”¾åœ¨äº†ä¸Šä¸€çº§çš„ 'data' æ–‡ä»¶å¤¹é‡Œ\n",
    "SOURCE_IMAGES_DIR = '../data' \n",
    "SOURCE_LABELS_FILE = '../data/boneage-train.csv'\n",
    "\n",
    "# NUM_SAMPLES å’Œç›®æ ‡è·¯å¾„çš„å®šä¹‰ä¿æŒä¸å˜\n",
    "NUM_SAMPLES = 5000 \n",
    "PROJECT_IMAGES_DIR = './data/ori_images'\n",
    "PROJECT_LABELS_FILE = './data/labels.csv'\n",
    "\n",
    "# --- ä¿®æ­£3ï¼šåœ¨å¤åˆ¶æ–‡ä»¶å‰ï¼Œå…ˆåˆ›å»ºå¥½ç›®æ ‡æ–‡ä»¶å¤¹ ---\n",
    "# è¿™æ ·å¯ä»¥ä»æ ¹æºä¸Šè§£å†³ \"OSError: Cannot save file into a non-existent directory\" çš„é—®é¢˜\n",
    "print(f\"\\næ­¥éª¤2ï¼šæ­£åœ¨å‡†å¤‡ç›®æ ‡æ–‡ä»¶å¤¹: {PROJECT_IMAGES_DIR}\")\n",
    "os.makedirs(PROJECT_IMAGES_DIR, exist_ok=True) # exist_ok=True è¡¨ç¤ºå¦‚æœæ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œä¹Ÿä¸ä¼šæŠ¥é”™\n",
    "# æ—¢ç„¶æˆ‘ä»¬ç”¨äº†makedirsï¼Œä¸‹é¢çš„if/elseé€»è¾‘å¯ä»¥ç®€åŒ–\n",
    "if os.listdir(PROJECT_IMAGES_DIR): # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©º\n",
    "    print(\"æ–‡ä»¶å¤¹éç©ºï¼Œæ­£åœ¨æ¸…ç©ºæ—§å›¾ç‰‡...\")\n",
    "    for file_name in os.listdir(PROJECT_IMAGES_DIR):\n",
    "        file_path = os.path.join(PROJECT_IMAGES_DIR, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    print(\"æ—§å›¾ç‰‡å·²æ¸…ç©ºã€‚\")\n",
    "else:\n",
    "    print(\"ç›®æ ‡æ–‡ä»¶å¤¹å·²å‡†å¤‡å°±ç»ªæˆ–ä¸ºç©ºã€‚\")\n",
    "\n",
    "# --- åé¢çš„æ­¥éª¤3å’Œ4ï¼Œå®Œå…¨ä¿ç•™ä½ åŸå§‹çš„ä»£ç é€»è¾‘ï¼Œä¸åšä»»ä½•ä¿®æ”¹ ---\n",
    "# --- 3. å¤„ç†å¹¶å¤åˆ¶æ ‡ç­¾æ–‡ä»¶ ---\n",
    "print(f\"\\næ­¥éª¤3ï¼šæ­£åœ¨ä» {SOURCE_LABELS_FILE} è¯»å–å¹¶å¤„ç†æ ‡ç­¾...\")\n",
    "try:\n",
    "    df_all = pd.read_csv(SOURCE_LABELS_FILE)\n",
    "    df_subset = df_all.head(NUM_SAMPLES)\n",
    "    if 'boneage' in df_subset.columns:\n",
    "         df_subset = df_subset.rename(columns={'boneage': 'y'})\n",
    "    df_subset.to_csv(PROJECT_LABELS_FILE, index=False)\n",
    "    print(f\"âœ… æˆåŠŸåˆ›å»ºå¹¶ä¿å­˜äº†åŒ…å« {len(df_subset)} æ¡è®°å½•çš„ {PROJECT_LABELS_FILE}\")\n",
    "    ids_to_copy = df_subset['id'].astype(str).tolist()\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ ‡ç­¾æ–‡ä»¶ {SOURCE_LABELS_FILE}ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚\"); ids_to_copy = []\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¯»å–æˆ–å¤„ç†æ ‡ç­¾æ–‡ä»¶æ—¶å‡ºé”™: {e}\"); ids_to_copy = []\n",
    "\n",
    "# --- 4. æ ¹æ®IDåˆ—è¡¨å¤åˆ¶å›¾ç‰‡æ–‡ä»¶ ---\n",
    "if ids_to_copy:\n",
    "    print(f\"\\næ­¥éª¤4ï¼šå‡†å¤‡æ ¹æ®æ ‡ç­¾æ–‡ä»¶å¤åˆ¶ {len(ids_to_copy)} å¼ å›¾ç‰‡...\"); copied_count = 0; not_found_count = 0\n",
    "    image_extension = '.png'\n",
    "    for image_id in ids_to_copy:\n",
    "        image_filename = image_id + image_extension\n",
    "        source_path = os.path.join(SOURCE_IMAGES_DIR, image_filename)\n",
    "        destination_path = os.path.join(PROJECT_IMAGES_DIR, image_filename)\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy2(source_path, destination_path); copied_count += 1\n",
    "        else:\n",
    "            image_filename_jpg = image_id + '.jpg'\n",
    "            source_path_jpg = os.path.join(SOURCE_IMAGES_DIR, image_filename_jpg)\n",
    "            if os.path.exists(source_path_jpg):\n",
    "                 shutil.copy2(source_path_jpg, os.path.join(PROJECT_IMAGES_DIR, image_filename_jpg)); copied_count += 1\n",
    "            else:\n",
    "                not_found_count += 1\n",
    "    print(\"\\n--- âœ… æ•°æ®å‡†å¤‡å®Œæˆ ---\")\n",
    "    print(f\"æˆåŠŸå¤åˆ¶äº† {copied_count} å¼ å›¾ç‰‡ã€‚\")\n",
    "    if not_found_count > 0: print(f\"æœ‰ {not_found_count} å¼ å›¾ç‰‡åœ¨æºæ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°ã€‚\")\n",
    "else:\n",
    "    print(\"\\nâŒ æ²¡æœ‰éœ€è¦å¤åˆ¶çš„å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶æ˜¯å¦æ­£ç¡®å¤„ç†ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MedicalCNN' from 'model' (/root/æ‰€æœ‰ä»£ç _forGPU/model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MedicalImageDataset, ECGDataset, \n\u001b[1;32m     26\u001b[0m     create_data_loaders, create_ecg_data_loaders, create_voice_data_loaders\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Import model classes\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MedicalCNN, ECG1DCNN, ModelTrainer, Voice1DCNN\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Import hyperparameter tuning\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhyperparameter_tuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HyperparameterTuner\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MedicalCNN' from 'model' (/root/æ‰€æœ‰ä»£ç _forGPU/model.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Import image processing functions\n",
    "from preprocessing import (\n",
    "    process_single_image, visualize_preprocessing, process_all_images,\n",
    "    # ECG processing functions\n",
    "    read_physionet_data, process_ecg_signal, process_all_ecg_signals, preprocess_jiachen_files,\n",
    "    visualize_ecg_preprocessing,\n",
    "    # voice processing functions\n",
    "    read_voice_data, read_voice_metadata, preprocess_voice_signal,\n",
    "    process_voice_file, process_all_voice_signals, create_voice_labels_file,\n",
    "    visualize_voice_preprocessing\n",
    ")\n",
    "\n",
    "# Import dataset functions\n",
    "from dataset import (\n",
    "    MedicalImageDataset, ECGDataset, \n",
    "    create_data_loaders, create_ecg_data_loaders, create_voice_data_loaders\n",
    ")\n",
    "\n",
    "# Import model classes\n",
    "from model import MedicalCNN, ECG1DCNN, ModelTrainer, Voice1DCNN\n",
    "\n",
    "# Import hyperparameter tuning\n",
    "from hyperparameter_tuning import HyperparameterTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the configuration for the experiment. You can modify these parameters to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name = '' # don't change this\n",
    "\n",
    "# Experiment configuration\n",
    "config = {\n",
    "    'data_type': 'image',  # 'image', 'voice' or 'ECG' - determines data processing pipeline\n",
    "    'task_type': 'regression',  # 'classification' or 'regression'\n",
    "    'num_classes': 2,  # for classification only\n",
    "    'image_dir': f'./data/{student_name}after_processed',\n",
    "    'labels_file': f'./data/{student_name}labels.csv',\n",
    "    'batch_size': 40,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    'random_seed': 42,\n",
    "    'early_stopping_patience': 15,\n",
    "    'save_dir': './results',\n",
    "    \n",
    "    # ECG-specific parameters\n",
    "    'ecg_max_length': 500,  # Target sequence length for ECG signals (uniform sampling)\n",
    "    \n",
    "    # voice-specific parameters\n",
    "    'voice_max_length': 500,  # 5 seconds at 8kHz\n",
    "    'target_variable': 'Voice Handicap Index (VHI) Score',  # 'VHI Score', 'RSI Score', 'Diagnosis'\n",
    "    \n",
    "    # Grid search parameters\n",
    "    'grid_search': {\n",
    "        'num_conv_layers': [3, 2],\n",
    "        'conv_channels': [64, 32],\n",
    "        'fc_layers': [[512], [64, 32]],  # , [1024, 256, 64]\n",
    "        'learning_rate': [0.01, 0.001,0.0003] # , 0.0001\n",
    "    }\n",
    "}\n",
    "\n",
    "# class_nms = {\n",
    "#     6: 'Frog',\n",
    "#     9: 'Truck',\n",
    "#     4: 'Deer',\n",
    "#     1: 'Automobile',\n",
    "#     2: 'Bird',\n",
    "#     7: 'Horse',\n",
    "#     8: 'Ship',\n",
    "#     3: 'Cat',\n",
    "#     5: 'Dog',\n",
    "#     0: 'Airplane'\n",
    "#     }\n",
    "\n",
    "# directly set the class names according to your data\n",
    "class_nms = {\n",
    "    0: 'Normal',\n",
    "    1: 'Abnormal'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's preprocess the data based on the data type (image or ECG signals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['data_type'] == 'image':\n",
    "    # Visualize preprocessing on a sample image\n",
    "    ori_image_dir = f'./data/{student_name}ori_images'\n",
    "    sample_image_path = list(Path(ori_image_dir).glob('*.png'))[0]\n",
    "    visualize_preprocessing(sample_image_path)\n",
    "    \n",
    "elif config['data_type'] == 'ECG':\n",
    "    label_df = pd.read_csv(config['labels_file'])\n",
    "    label_df.columns = [x.strip() for x in label_df.columns]\n",
    "    label_df['y'] = label_df['cause of death'].astype(int)\n",
    "    # label_df['id'] = label_df['id'].apply(lambda x: x.replace('Jiachen_', 'P'))\n",
    "    label_df.to_csv(config['labels_file'], index=False)\n",
    "    \n",
    "    # ECG data directories\n",
    "    ecg_raw_dir = f'./data/{student_name}ori_images'\n",
    "    ecg_processed_dir = f'./data/{student_name}after_processed'\n",
    "    \n",
    "    # Handle Jiachen's special file naming\n",
    "    preprocess_jiachen_files(ecg_raw_dir, config['labels_file'])\n",
    "    \n",
    "    # Process all ECG files\n",
    "    processed_count = process_all_ecg_signals(\n",
    "        ecg_raw_dir, \n",
    "        ecg_processed_dir, \n",
    "        config['ecg_max_length']\n",
    "    )\n",
    "    \n",
    "    # Visualize preprocessing on a sample ECG\n",
    "    sample_files = list(Path(ecg_raw_dir).glob('*.dat'))\n",
    "    if sample_files:\n",
    "        sample_dat = sample_files[0]\n",
    "        sample_hea = Path(str(sample_dat).replace('.dat', '.hea'))\n",
    "        if sample_hea.exists():\n",
    "            visualize_ecg_preprocessing(sample_dat, sample_hea, config['ecg_max_length'])\n",
    "        else:\n",
    "            print(f\"Warning: Header file {sample_hea} not found\")\n",
    "    else:\n",
    "        print(\"No ECG files found for visualization\")\n",
    "\n",
    "elif config['data_type'] == 'voice':\n",
    "    # voice data directories\n",
    "    voice_raw_dir = f'./data/{student_name}ori_images'\n",
    "    voice_processed_dir = f'./data/{student_name}after_processed'\n",
    "    \n",
    "    # Process all voice files\n",
    "    processed_count = process_all_voice_signals(\n",
    "        voice_raw_dir, \n",
    "        voice_processed_dir, \n",
    "        config['voice_max_length']\n",
    "    )\n",
    "\n",
    "    # Create labels file\n",
    "    labels_df = create_voice_labels_file(\n",
    "        voice_raw_dir, \n",
    "        config['labels_file'], \n",
    "        config['target_variable']\n",
    "    )\n",
    "    print(labels_df.head())\n",
    "    labels_df.to_csv(config['labels_file'], index=False)\n",
    "\n",
    "    # Visualize preprocessing on a sample voice signal\n",
    "    sample_files = list(Path(voice_raw_dir).glob('*.dat'))\n",
    "    if sample_files:\n",
    "        sample_dat = sample_files[0]\n",
    "        sample_hea = Path(str(sample_dat).replace('.dat', '.hea'))\n",
    "        sample_txt = Path(str(sample_dat).replace('.dat', '.txt'))\n",
    "        if sample_hea.exists() and sample_txt.exists():\n",
    "            visualize_voice_preprocessing(sample_txt, sample_hea, config['voice_max_length'], 3)\n",
    "        else:\n",
    "            print(f\"Warning: Header file {sample_hea} not found\")\n",
    "    else:\n",
    "        print(\"No voice files found for visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process all images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['data_type'] == 'image':\n",
    "    # Process all images\n",
    "    input_dir = ori_image_dir\n",
    "    output_dir = config['image_dir']\n",
    "    target_size = (224, 224)  # Standard size for many CNN architectures\n",
    "\n",
    "    process_all_images(input_dir, output_dir, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n--- åˆ›å»ºç›®æ ‡å€¼æ ‡å‡†åŒ–ç¼©æ”¾å™¨ ---\")\n",
    "\n",
    "# 1. åŠ è½½æ ‡ç­¾æ–‡ä»¶ï¼Œåªä¸ºäº†åˆ›å»ºscaler\n",
    "labels_df = pd.read_csv(config['labels_file'])\n",
    "if 'boneage' in labels_df.columns:\n",
    "    labels_df = labels_df.rename(columns={'boneage': 'y'})\n",
    "\n",
    "# 2. ä»…åœ¨è®­ç»ƒé›†éƒ¨åˆ†ä¸Šfit scaler\n",
    "train_val_df, _ = train_test_split(labels_df, test_size=config['test_ratio'], random_state=config['random_seed'])\n",
    "train_df, _ = train_test_split(train_val_df, test_size=config['val_ratio'] / (config['train_ratio'] + config['val_ratio']), random_state=config['random_seed'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df['y'].values.reshape(-1, 1))\n",
    "\n",
    "# 3. æŠŠscalerå­˜åˆ°configé‡Œï¼Œç»™åé¢çš„æ­¥éª¤ç”¨\n",
    "config['scaler'] = scaler\n",
    "print(\"âœ… æ ‡å‡†åŒ–ç¼©æ”¾å™¨å·²åˆ›å»ºå¹¶ä¿å­˜åˆ°configä¸­ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Create data loaders for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create data loaders based on data type\n",
    "if config['data_type'] == 'image':\n",
    "    data_loaders = create_data_loaders(\n",
    "        data_dir=config['image_dir'],\n",
    "        labels_file=config['labels_file'],\n",
    "        task_type=config['task_type'],\n",
    "        batch_size=config['batch_size'],\n",
    "        train_ratio=config['train_ratio'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        test_ratio=config['test_ratio'],\n",
    "        random_seed=config['random_seed'],\n",
    "        scaler=config.get('scaler', None)\n",
    "    )\n",
    "elif config['data_type'] == 'ECG':\n",
    "    # For ECG data, use processed ECG data directory\n",
    "    data_loaders = create_ecg_data_loaders(\n",
    "        data_dir=config['image_dir'],\n",
    "        labels_file=config['labels_file'],\n",
    "        task_type=config['task_type'],\n",
    "        batch_size=config['batch_size'],\n",
    "        train_ratio=config['train_ratio'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        test_ratio=config['test_ratio'],\n",
    "        random_seed=config['random_seed']\n",
    "    )\n",
    "elif config['data_type'] == 'voice':\n",
    "    print('Creating voice data loaders')\n",
    "    data_loaders = create_voice_data_loaders(\n",
    "        data_dir=config['image_dir'],\n",
    "        labels_file=config['labels_file'],\n",
    "        task_type=config['task_type'],\n",
    "        batch_size=config['batch_size'],\n",
    "        train_ratio=config['train_ratio'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        test_ratio=config['test_ratio'],\n",
    "        random_seed=config['random_seed'],\n",
    "        target_length=config['voice_max_length']\n",
    "    )\n",
    "\n",
    "train_loader, val_loader, test_loader = data_loaders['train'], data_loaders['val'], data_loaders['test']\n",
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(data_loaders['train'].dataset)}\")\n",
    "print(f\"Validation set size: {len(data_loaders['val'].dataset)}\")\n",
    "print(f\"Test set size: {len(data_loaders['test'].dataset)}\")\n",
    "print(f\"Data type: {config['data_type']}\")\n",
    "\n",
    "# Show sample data shape\n",
    "sample_data, sample_label = next(iter(train_loader))\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "print(f\"Sample label shape: {sample_label.shape}\")\n",
    "sample_data[:3,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Perform grid search to find the best model architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     24\u001b[0m tuner \u001b[38;5;241m=\u001b[39m HyperparameterTuner(\n\u001b[1;32m     25\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     26\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39mPath(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_search\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m grid_search_results \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mgrid_search(\n\u001b[1;32m     37\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_search\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     38\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     39\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_patience\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m     43\u001b[0m tuner\u001b[38;5;241m.\u001b[39mplot_results()\n",
      "File \u001b[0;32m~/Desktop/AI_medicine-main/hyperparameter_tuning.py:129\u001b[0m, in \u001b[0;36mHyperparameterTuner.grid_search\u001b[0;34m(self, param_grid, num_epochs, early_stopping_patience)\u001b[0m\n\u001b[1;32m    120\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\n\u001b[1;32m    121\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    122\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type\n\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m history \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    130\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader,\n\u001b[1;32m    131\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader,\n\u001b[1;32m    132\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[1;32m    133\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombination_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    134\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39mearly_stopping_patience\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m trainer\u001b[38;5;241m.\u001b[39mplot_training_history(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombination_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Get best validation metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AI_medicine-main/model.py:605\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, train_loader, val_loader, num_epochs, save_dir, early_stopping_patience)\u001b[0m\n\u001b[1;32m    601\u001b[0m epoch_pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epoch_pbar:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m     train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch(train_loader)\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metrics\u001b[38;5;241m.\u001b[39mappend(train_metrics)\n",
      "File \u001b[0;32m~/Desktop/AI_medicine-main/model.py:436\u001b[0m, in \u001b[0;36mModelTrainer.train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 436\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# Gradient clipping to prevent gradient explosion\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path # ç¡®ä¿Pathå·²å¯¼å…¥\n",
    "\n",
    "def convert_types_for_json(obj):\n",
    "    \"\"\"ä¸€ä¸ªä¸‡èƒ½çš„ç±»å‹è½¬æ¢å‡½æ•°\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_types_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_types_for_json(i) for i in obj]\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    return obj\n",
    "    \n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Create universal hyperparameter tuner\n",
    "print(f\"Performing {config['data_type']} grid search...\")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    task_type=config['task_type'],\n",
    "    model_type=config['data_type'],  # 'image' or 'ECG' or 'voice'\n",
    "    num_classes=config['num_classes'],\n",
    "    input_length=config.get('ecg_max_length', 5000) if config['data_type'] == 'ECG' else config.get('voice_max_length', 5000) if config['data_type'] == 'voice' else None, \n",
    "    device=device,\n",
    "    save_dir=Path(config['save_dir']) / 'grid_search'\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_results = tuner.grid_search(\n",
    "    param_grid=config['grid_search'],\n",
    "    num_epochs=config['num_epochs'],\n",
    "    early_stopping_patience=config['early_stopping_patience']\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "tuner.plot_results()\n",
    "\n",
    "# Print best combination/\n",
    "print('\\nBest combination:')\n",
    "print(json.dumps(grid_search_results['best_combination'], indent=2))\n",
    "print('\\nBest validation metrics:')\n",
    "print(json.dumps(grid_search_results['best_val_metrics'], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Best Model\n",
    "\n",
    "Train the model with the best hyperparameters found during grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# âœ…âœ…âœ… Train Best Model å•å…ƒæ ¼çš„æœ€ç»ˆã€ç»å¯¹æ­£ç¡®çš„ä»£ç  âœ…âœ…âœ…\n",
    "# =======================================================\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹ä¿å­˜çš„è·¯å¾„\n",
    "best_model_path = Path(config['save_dir']) / 'best_model' / 'best_model.pth'\n",
    "\n",
    "# --- æ ¸å¿ƒé€»è¾‘å¼€å§‹ ---\n",
    "\n",
    "# 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "if best_model_path.exists():\n",
    "    print(f\"âœ… æ‰¾åˆ°äº†å·²ç»è®­ç»ƒå¥½çš„æœ€ä½³æ¨¡å‹: {best_model_path}\")\n",
    "    print(\"æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡å’Œé…ç½®...\")\n",
    "\n",
    "    # å…ˆæŠŠæ¨¡å‹æ–‡ä»¶åŠ è½½åˆ°CPUä¸Šï¼Œè¿™æ ·æ›´å®‰å…¨\n",
    "    checkpoint = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # ã€å…³é”®ã€‘ä»checkpointä¸­æ¢å¤å½“æ—¶è®­ç»ƒæ¨¡å‹çš„å‚æ•°ï¼\n",
    "    # è¿™ç¡®ä¿äº†æˆ‘ä»¬åˆ›å»ºçš„æ¨¡å‹ç»“æ„å’ŒåŠ è½½çš„æƒé‡100%åŒ¹é…\n",
    "    # æˆ‘ä»¬éœ€è¦ä»model_state_dictçš„é”®åæ¥åæ¨å‚æ•°ï¼Œè¿™æœ‰ç‚¹å¤æ‚ï¼Œ\n",
    "    # ä¸€ä¸ªæ›´ç¨³å¦¥çš„æ–¹æ³•æ˜¯å‡è®¾Grid Searchçš„ç»“æœæ˜¯ç¨³å®šçš„ã€‚\n",
    "    # æˆ‘ä»¬ä»ç„¶ä»jsonæ–‡ä»¶è¯»å–æœ€ä½³å‚æ•°ï¼Œè¿™æ˜¯æœ€å¯é çš„æºå¤´ã€‚\n",
    "    grid_search_res_path = 'results/grid_search/grid_search_summary.json'\n",
    "    with open(grid_search_res_path, 'r') as f:\n",
    "        grid_search_results = json.load(f)\n",
    "    best_params = grid_search_results['best_combination']\n",
    "    print(\"å°†ä½¿ç”¨Grid Searchæ‰¾åˆ°çš„æœ€ä½³å‚æ•°æ¥åˆ›å»ºæ¨¡å‹ç»“æ„:\")\n",
    "    print(best_params)\n",
    "\n",
    "    # ç”¨è¿™äº›ã€ç¡®å®šã€‘çš„å‚æ•°åˆ›å»ºæ¨¡å‹\n",
    "    if config['data_type'] == 'image':\n",
    "        model = MedicalCNN(\n",
    "            task_type=config['task_type'],\n",
    "            num_classes=config['num_classes'],\n",
    "            num_conv_layers=best_params['num_conv_layers'],\n",
    "            conv_channels=best_params['conv_channels'],\n",
    "            fc_layers=best_params['fc_layers']\n",
    "        )\n",
    "    # ... (elif ECG, elif voice çš„éƒ¨åˆ†ä¿æŒä¸å˜) ...\n",
    "\n",
    "    # ç°åœ¨ï¼ŒæŠŠåŠ è½½çš„æƒé‡åº”ç”¨åˆ°è¿™ä¸ªç»“æ„æ­£ç¡®çš„æ¨¡å‹ä¸Š\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # åˆå§‹åŒ–Trainerï¼Œè¿™æ¬¡æˆ‘ä»¬åªåŠ è½½æ¨¡å‹ï¼Œä¸è¿›è¡Œè®­ç»ƒ\n",
    "    trainer = ModelTrainer(model=model, criterion=None, optimizer=None, device=device, task_type=config['task_type'])\n",
    "    history = None # å› ä¸ºæ˜¯åŠ è½½ï¼Œæ‰€ä»¥æ²¡æœ‰æ–°çš„è®­ç»ƒå†å²\n",
    "    print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å°†è·³è¿‡è®­ç»ƒï¼Œç›´æ¥è¿›å…¥è¯„ä¼°ã€‚\")\n",
    "\n",
    "else:\n",
    "    # åªæœ‰åœ¨æ‰¾ä¸åˆ°å·²ä¿å­˜æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ‰ä»å¤´è®­ç»ƒ\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æœ€ä½³æ¨¡å‹ï¼Œç°åœ¨å°†ä½¿ç”¨Grid Searchæ‰¾åˆ°çš„æœ€ä½³å‚æ•°ä»å¤´å¼€å§‹è®­ç»ƒ...\")\n",
    "\n",
    "    # ä»jsonæ–‡ä»¶è¯»å–æœ€ä½³å‚æ•°\n",
    "    grid_search_res_path = 'results/grid_search/grid_search_summary.json'\n",
    "    with open(grid_search_res_path, 'r') as f:\n",
    "        grid_search_results = json.load(f)\n",
    "    best_params = grid_search_results['best_combination']\n",
    "    print(\"å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è¿›è¡Œè®­ç»ƒ:\")\n",
    "    print(best_params)\n",
    "\n",
    "    # ç”¨æœ€ä½³å‚æ•°åˆ›å»ºæ¨¡å‹\n",
    "    if config['data_type'] == 'image':\n",
    "        model = MedicalCNN(\n",
    "            task_type=config['task_type'],\n",
    "            num_classes=config['num_classes'],\n",
    "            num_conv_layers=best_params['num_conv_layers'],\n",
    "            conv_channels=best_params['conv_channels'],\n",
    "            fc_layers=best_params['fc_layers']\n",
    "        )\n",
    "    # ... (elif ECG, elif voice çš„éƒ¨åˆ†ä¿æŒä¸å˜) ...\n",
    "\n",
    "    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "    criterion = nn.MSELoss() if config['task_type'] == 'regression' else nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    # åˆå§‹åŒ–Trainer\n",
    "    trainer = ModelTrainer(model=model, criterion=criterion, optimizer=optimizer, device=device, task_type=config['task_type'])\n",
    "\n",
    "    # å¼€å§‹è®­ç»ƒ\n",
    "    history = trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=100, # åœ¨è¿™é‡Œç”¨ä½ è¯´çš„100ä¸ªepochæ¥è®­ç»ƒ\n",
    "        save_dir=Path(config['save_dir']) / 'best_model',\n",
    "        early_stopping_patience=15\n",
    "    )\n",
    "    # ç»˜åˆ¶è®­ç»ƒå†å²\n",
    "    trainer.plot_training_history(Path(config['save_dir']) / 'best_model')\n",
    "\n",
    "# --- åç»­ä»£ç ä¿æŒä¸å˜ ---\n",
    "\n",
    "# Set class names for classification tasks\n",
    "if config['task_type'] == 'classification':\n",
    "    class_names = class_nms\n",
    "    trainer.set_class_names(class_names)\n",
    "    print(f\"Class names set: {class_names}\")\n",
    "\n",
    "# Save the trainer state for later use\n",
    "best_trainer = trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# âœ…âœ…âœ… æœ€åçš„ã€ç»Ÿä¸€å˜é‡åçš„Evaluationè„šæœ¬ âœ…âœ…âœ…\n",
    "# =======================================================\n",
    "\n",
    "# --- æ‰‹åŠ¨è®¡ç®—çœŸå®çš„MAE ---\n",
    "print(\"\\n--- è®¡ç®—çœŸå®çš„å¹³å‡ç»å¯¹è¯¯å·® (MAE) ---\")\n",
    "\n",
    "# 1. æ£€æŸ¥scalerå’Œtraineræ˜¯å¦å­˜åœ¨\n",
    "if 'scaler' in config and 'trainer' in locals():\n",
    "    # 2. ç›´æ¥ä½¿ç”¨ä¸Šä¸€ä¸ªå•å…ƒæ ¼åˆ›å»ºçš„'trainer'å¯¹è±¡ï¼\n",
    "    trainer.model.eval() # <--- ä¿®æ”¹è¿™é‡Œ\n",
    "\n",
    "    # 3. è·å–æ‰€æœ‰åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹å€¼\n",
    "    true_labels_scaled, all_preds_scaled, _ = trainer.get_predictions_and_probabilities(test_loader) # <--- ä¿®æ”¹è¿™é‡Œ\n",
    "\n",
    "    # 4. ä½¿ç”¨scalerè¿›è¡Œåå‘è½¬æ¢\n",
    "    scaler = config['scaler']\n",
    "    predicted_boneage_real = scaler.inverse_transform(all_preds_scaled.reshape(-1, 1))\n",
    "    true_boneage_real = scaler.inverse_transform(true_labels_scaled.reshape(-1, 1))\n",
    "\n",
    "    # 5. è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆçš„MAE\n",
    "    final_mae = np.mean(np.abs(true_boneage_real - predicted_boneage_real))\n",
    "    print(f\"\\nâœ… æœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šçš„çœŸå®MAE: {final_mae:.2f} ä¸ªæœˆ\")\n",
    "    \n",
    "    # æŠŠè®¡ç®—å‡ºçš„çœŸå®MAEå­˜èµ·æ¥ï¼Œæ–¹ä¾¿ä¸‹é¢ç”¨\n",
    "    real_mae_value = final_mae\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°scaleræˆ–trainerï¼Œæ— æ³•è®¡ç®—çœŸå®MAEã€‚\")\n",
    "    real_mae_value = -1 # è®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼\n",
    "\n",
    "\n",
    "# --- è¿›è¡Œä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯” ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING MODEL COMPARISON WITH TRADITIONAL ML METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # è°ƒç”¨å¯¹æ¯”å‡½æ•°ï¼Œç›´æ¥ç”¨'trainer'\n",
    "    comparison_results = compare_models_performance(\n",
    "        best_cnn_trainer=trainer, # <--- è¿™é‡Œæœ¬æ¥å°±æ˜¯å¯¹çš„\n",
    "        # ... å…¶ä»–å‚æ•°ä¿æŒä¸å˜ ...\n",
    "    )\n",
    "\n",
    "    # Display comparison results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for model_name, metrics in comparison_results.items():\n",
    "        # æŠŠæˆ‘ä»¬åˆšåˆšè®¡ç®—çš„çœŸå®MAEæ›´æ–°åˆ°ç»“æœé‡Œ\n",
    "        if model_name == 'Deep Learning (CNN)' and real_mae_value != -1:\n",
    "            metrics['mae'] = real_mae_value\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            if metric_name.lower() == 'mae' and model_name == 'Deep Learning (CNN)':\n",
    "                print(f\"  çœŸå®MAE: {value:.4f} ä¸ªæœˆ\")\n",
    "            else:\n",
    "                print(f\"  {metric_name.upper()}: {value:.4f}\")\n",
    "\n",
    "    print(f\"\\nâœ… æ‰€æœ‰å¯¹æ¯”å›¾å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜åˆ°: ./results/model_evaluation/\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\nâŒ åœ¨æ¨¡å‹å¯¹æ¯”ç¯èŠ‚å‡ºé”™äº†: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison with traditional machine learning methods\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING MODEL COMPARISON WITH TRADITIONAL ML METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import required modules for comparison\n",
    "from model import compare_models_performance\n",
    "\n",
    "# DEBUG CONTROL - Set to True for detailed debugging information\n",
    "DEBUG_MODE = False  # Change to True if you want to see detailed debug information\n",
    "\n",
    "# Set class names for comparison (if classification task)\n",
    "comparison_class_names = None\n",
    "if config['task_type'] == 'classification':\n",
    "    comparison_class_names = class_names  # Modify according to your classes\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Task type: {config['task_type']}\")\n",
    "print(f\"  Train set size: {len(train_loader.dataset)}\")\n",
    "print(f\"  Validation set size: {len(val_loader.dataset)}\")\n",
    "print(f\"  Test set size: {len(test_loader.dataset)}\")\n",
    "print(f\"  Class names: {comparison_class_names}\")\n",
    "print(f\"  Debug mode: {DEBUG_MODE}\")\n",
    "print(f\"  Save directory: ./results/model_evaluation\")\n",
    "\n",
    "try:\n",
    "    # Compare models performance\n",
    "    comparison_results = compare_models_performance(\n",
    "        best_cnn_trainer=trainer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader,\n",
    "        save_dir='./results/model_evaluation',\n",
    "        task_type=config['task_type'],\n",
    "        class_names=comparison_class_names,\n",
    "        debug=DEBUG_MODE  # Control debug output\n",
    "    )\n",
    "\n",
    "    # Display comparison results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for model_name, metrics in comparison_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name.upper()}: {value:.4f}\")\n",
    "\n",
    "    print(f\"\\nAll comparison plots saved to: ./results/model_evaluation/\")\n",
    "    print(\"Generated files:\")\n",
    "    if config['task_type'] == 'classification':\n",
    "        print(\"- aupr_comparison.png (AUPR curves)\")\n",
    "        print(\"- auc_comparison.png (ROC curves)\")\n",
    "        print(\"- accuracy_comparison.png\")\n",
    "        print(\"- precision_comparison.png\") \n",
    "        print(\"- recall_comparison.png\")\n",
    "        print(\"- f1_comparison.png\")\n",
    "    else:\n",
    "        print(\"- mse_comparison.png\")\n",
    "        print(\"- mae_comparison.png\")\n",
    "        print(\"- r2_comparison.png\")\n",
    "    print(\"- model_comparison_results.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during model comparison: {e}\")\n",
    "    if DEBUG_MODE:\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
